{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomor 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99908895, 0.98201379, 0.99966465, 0.99330715, 0.99987661,\n",
       "       0.99330715, 0.99908895, 0.99987661, 0.99966465, 0.99987661])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    sigmoid = 1/(1+np.exp(-z))\n",
    "    return sigmoid\n",
    "\n",
    "z = np.random.randint(0,10,size=10)\n",
    "sigmoid(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nomor 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Generate dummy data\n",
    "x = np.random.randint(0, 10, size=10)\n",
    "y = np.round(4 + 3 * x + np.random.randn(10), 2)\n",
    "\n",
    "w = np.random.randn(1,1)  \n",
    "b = np.zeros(1)  \n",
    "\n",
    "x = x.reshape(10,1)\n",
    "y = y.reshape(10,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5],\n",
       "       [0],\n",
       "       [3],\n",
       "       [3],\n",
       "       [7],\n",
       "       [9],\n",
       "       [3],\n",
       "       [5],\n",
       "       [2],\n",
       "       [4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[18.71],\n",
       "       [ 3.87],\n",
       "       [12.83],\n",
       "       [11.24],\n",
       "       [24.91],\n",
       "       [32.37],\n",
       "       [14.13],\n",
       "       [18.64],\n",
       "       [11.22],\n",
       "       [14.66]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits(w, b, x):\n",
    "    y_hat = x @ w + b  # Perform matrix multiplication correctly\n",
    "    return y_hat\n",
    "\n",
    "y_hat = fits(w, b, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126.36228689239384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_function(y_hat, y):\n",
    "    m = len(y)\n",
    "    loss = (1 / (2 * m)) * np.sum((y_hat - y) ** 2)\n",
    "    return loss\n",
    "\n",
    "loss_function(y_hat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turunan(y_hat, y, x):\n",
    "    m = len(y)\n",
    "    dw = (1 / m) * np.dot((y_hat - y).T, x)\n",
    "    db = (1 / m) * np.sum((y_hat - y))\n",
    "    return dw, db\n",
    "\n",
    "dw, db = turunan(y_hat, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.17926261]]), array([0.14501669]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_parameter(w, b, dw, db, learning_rate=0.01):\n",
    "    w = w - learning_rate * dw\n",
    "    b = b - learning_rate * db\n",
    "    return w, b\n",
    "\n",
    "update_parameter(w,b,dw,db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Loss = 126.36228689239384\n",
      "Iteration 1: Loss = 74.73233029958168\n",
      "Iteration 2: Loss = 44.47429016160575\n",
      "Iteration 3: Loss = 26.74038486482908\n",
      "Iteration 4: Loss = 16.345740012096247\n",
      "Iteration 5: Loss = 10.251975599068086\n",
      "Iteration 6: Loss = 6.6785753636601335\n",
      "Iteration 7: Loss = 4.582141761472458\n",
      "Iteration 8: Loss = 3.3512350078101423\n",
      "Iteration 9: Loss = 2.6275461933513604\n",
      "Iteration 10: Loss = 2.201103228014482\n",
      "Iteration 11: Loss = 1.9488606439291456\n",
      "Iteration 12: Loss = 1.7987127042101552\n",
      "Iteration 13: Loss = 1.708404782454489\n",
      "Iteration 14: Loss = 1.6531752540081026\n",
      "Iteration 15: Loss = 1.6185135404547983\n",
      "Iteration 16: Loss = 1.5959162628622137\n",
      "Iteration 17: Loss = 1.5804003119397316\n",
      "Iteration 18: Loss = 1.5690454862625742\n",
      "Iteration 19: Loss = 1.5601404660297342\n",
      "Iteration 20: Loss = 1.5526823465119524\n",
      "Iteration 21: Loss = 1.5460833604475421\n",
      "Iteration 22: Loss = 1.5399990149017468\n",
      "Iteration 23: Loss = 1.5342273780214726\n",
      "Iteration 24: Loss = 1.5286500602262842\n",
      "Iteration 25: Loss = 1.5231976286892406\n",
      "Iteration 26: Loss = 1.5178293399307679\n",
      "Iteration 27: Loss = 1.5125212629470743\n",
      "Iteration 28: Loss = 1.5072593192622423\n",
      "Iteration 29: Loss = 1.5020352043389265\n",
      "Iteration 30: Loss = 1.4968439974893821\n",
      "Iteration 31: Loss = 1.491682761261894\n",
      "Iteration 32: Loss = 1.486549720668083\n",
      "Iteration 33: Loss = 1.4814437822018753\n",
      "Iteration 34: Loss = 1.476364251979212\n",
      "Iteration 35: Loss = 1.471310670564222\n",
      "Iteration 36: Loss = 1.4662827161746261\n",
      "Iteration 37: Loss = 1.4612801479579676\n",
      "Iteration 38: Loss = 1.4563027727496796\n",
      "Iteration 39: Loss = 1.4513504255917236\n",
      "Iteration 40: Loss = 1.4464229583150283\n",
      "Iteration 41: Loss = 1.4415202328474113\n",
      "Iteration 42: Loss = 1.436642117290659\n",
      "Iteration 43: Loss = 1.4317884836203776\n",
      "Iteration 44: Loss = 1.426959206336801\n",
      "Iteration 45: Loss = 1.4221541616728777\n",
      "Iteration 46: Loss = 1.417373227128931\n",
      "Iteration 47: Loss = 1.4126162811986958\n",
      "Iteration 48: Loss = 1.4078832032075328\n",
      "Iteration 49: Loss = 1.403173873216347\n",
      "Iteration 50: Loss = 1.398488171964056\n",
      "Iteration 51: Loss = 1.3938259808326259\n",
      "Iteration 52: Loss = 1.3891871818253545\n",
      "Iteration 53: Loss = 1.3845716575529143\n",
      "Iteration 54: Loss = 1.3799792912239552\n",
      "Iteration 55: Loss = 1.375409966638388\n",
      "Iteration 56: Loss = 1.3708635681822274\n",
      "Iteration 57: Loss = 1.3663399808233834\n",
      "Iteration 58: Loss = 1.3618390901079827\n",
      "Iteration 59: Loss = 1.3573607821570384\n",
      "Iteration 60: Loss = 1.3529049436633012\n",
      "Iteration 61: Loss = 1.3484714618882452\n",
      "Iteration 62: Loss = 1.3440602246591218\n",
      "Iteration 63: Loss = 1.3396711203660714\n",
      "Iteration 64: Loss = 1.3353040379592684\n",
      "Iteration 65: Loss = 1.3309588669460943\n",
      "Iteration 66: Loss = 1.3266354973883292\n",
      "Iteration 67: Loss = 1.3223338198993686\n",
      "Iteration 68: Loss = 1.3180537256414453\n",
      "Iteration 69: Loss = 1.3137951063228828\n",
      "Iteration 70: Loss = 1.309557854195344\n",
      "Iteration 71: Loss = 1.3053418620511106\n",
      "Iteration 72: Loss = 1.3011470232203708\n",
      "Iteration 73: Loss = 1.296973231568515\n",
      "Iteration 74: Loss = 1.2928203814934558\n",
      "Iteration 75: Loss = 1.288688367922954\n",
      "Iteration 76: Loss = 1.2845770863119568\n",
      "Iteration 77: Loss = 1.2804864326399577\n",
      "Iteration 78: Loss = 1.2764163034083635\n",
      "Iteration 79: Loss = 1.2723665956378734\n",
      "Iteration 80: Loss = 1.268337206865874\n",
      "Iteration 81: Loss = 1.2643280351438504\n",
      "Iteration 82: Loss = 1.2603389790348\n",
      "Iteration 83: Loss = 1.2563699376106752\n",
      "Iteration 84: Loss = 1.25242081044982\n",
      "Iteration 85: Loss = 1.2484914976344366\n",
      "Iteration 86: Loss = 1.2445818997480533\n",
      "Iteration 87: Loss = 1.2406919178730123\n",
      "Iteration 88: Loss = 1.236821453587963\n",
      "Iteration 89: Loss = 1.2329704089653748\n",
      "Iteration 90: Loss = 1.2291386865690583\n",
      "Iteration 91: Loss = 1.2253261894517002\n",
      "Iteration 92: Loss = 1.2215328211524124\n",
      "Iteration 93: Loss = 1.2177584856942874\n",
      "Iteration 94: Loss = 1.2140030875819718\n",
      "Iteration 95: Loss = 1.2102665317992523\n",
      "Iteration 96: Loss = 1.2065487238066481\n",
      "Iteration 97: Loss = 1.2028495695390236\n",
      "Iteration 98: Loss = 1.1991689754032009\n",
      "Iteration 99: Loss = 1.1955068482756006\n",
      "Iteration 100: Loss = 1.1918630954998786\n",
      "Iteration 101: Loss = 1.1882376248845858\n",
      "Iteration 102: Loss = 1.184630344700836\n",
      "Iteration 103: Loss = 1.1810411636799794\n",
      "Iteration 104: Loss = 1.177469991011301\n",
      "Iteration 105: Loss = 1.173916736339717\n",
      "Iteration 106: Loss = 1.170381309763493\n",
      "Iteration 107: Loss = 1.1668636218319672\n",
      "Iteration 108: Loss = 1.1633635835432852\n",
      "Iteration 109: Loss = 1.1598811063421528\n",
      "Iteration 110: Loss = 1.156416102117592\n",
      "Iteration 111: Loss = 1.152968483200713\n",
      "Iteration 112: Loss = 1.1495381623624958\n",
      "Iteration 113: Loss = 1.1461250528115834\n",
      "Iteration 114: Loss = 1.1427290681920848\n",
      "Iteration 115: Loss = 1.1393501225813913\n",
      "Iteration 116: Loss = 1.135988130488003\n",
      "Iteration 117: Loss = 1.1326430068493636\n",
      "Iteration 118: Loss = 1.1293146670297103\n",
      "Iteration 119: Loss = 1.1260030268179317\n",
      "Iteration 120: Loss = 1.1227080024254368\n",
      "Iteration 121: Loss = 1.1194295104840364\n",
      "Iteration 122: Loss = 1.1161674680438312\n",
      "Iteration 123: Loss = 1.1129217925711155\n",
      "Iteration 124: Loss = 1.1096924019462877\n",
      "Iteration 125: Loss = 1.1064792144617728\n",
      "Iteration 126: Loss = 1.1032821488199571\n",
      "Iteration 127: Loss = 1.1001011241311276\n",
      "Iteration 128: Loss = 1.0969360599114273\n",
      "Iteration 129: Loss = 1.093786876080819\n",
      "Iteration 130: Loss = 1.0906534929610594\n",
      "Iteration 131: Loss = 1.0875358312736825\n",
      "Iteration 132: Loss = 1.0844338121379937\n",
      "Iteration 133: Loss = 1.0813473570690757\n",
      "Iteration 134: Loss = 1.0782763879757986\n",
      "Iteration 135: Loss = 1.0752208271588486\n",
      "Iteration 136: Loss = 1.072180597308761\n",
      "Iteration 137: Loss = 1.069155621503962\n",
      "Iteration 138: Loss = 1.066145823208825\n",
      "Iteration 139: Loss = 1.0631511262717328\n",
      "Iteration 140: Loss = 1.060171454923151\n",
      "Iteration 141: Loss = 1.0572067337737117\n",
      "Iteration 142: Loss = 1.0542568878123042\n",
      "Iteration 143: Loss = 1.051321842404181\n",
      "Iteration 144: Loss = 1.0484015232890642\n",
      "Iteration 145: Loss = 1.045495856579271\n",
      "Iteration 146: Loss = 1.042604768757841\n",
      "Iteration 147: Loss = 1.0397281866766803\n",
      "Iteration 148: Loss = 1.0368660375547056\n",
      "Iteration 149: Loss = 1.0340182489760064\n",
      "Iteration 150: Loss = 1.031184748888012\n",
      "Iteration 151: Loss = 1.0283654655996672\n",
      "Iteration 152: Loss = 1.02556032777962\n",
      "Iteration 153: Loss = 1.0227692644544166\n",
      "Iteration 154: Loss = 1.0199922050067047\n",
      "Iteration 155: Loss = 1.0172290791734477\n",
      "Iteration 156: Loss = 1.0144798170441458\n",
      "Iteration 157: Loss = 1.0117443490590705\n",
      "Iteration 158: Loss = 1.0090226060074996\n",
      "Iteration 159: Loss = 1.0063145190259688\n",
      "Iteration 160: Loss = 1.003620019596531\n",
      "Iteration 161: Loss = 1.0009390395450202\n",
      "Iteration 162: Loss = 0.9982715110393268\n",
      "Iteration 163: Loss = 0.9956173665876824\n",
      "Iteration 164: Loss = 0.9929765390369513\n",
      "Iteration 165: Loss = 0.9903489615709331\n",
      "Iteration 166: Loss = 0.9877345677086693\n",
      "Iteration 167: Loss = 0.9851332913027637\n",
      "Iteration 168: Loss = 0.9825450665377087\n",
      "Iteration 169: Loss = 0.9799698279282167\n",
      "Iteration 170: Loss = 0.9774075103175692\n",
      "Iteration 171: Loss = 0.974858048875963\n",
      "Iteration 172: Loss = 0.9723213790988723\n",
      "Iteration 173: Loss = 0.9697974368054161\n",
      "Iteration 174: Loss = 0.967286158136735\n",
      "Iteration 175: Loss = 0.9647874795543743\n",
      "Iteration 176: Loss = 0.962301337838677\n",
      "Iteration 177: Loss = 0.9598276700871851\n",
      "Iteration 178: Loss = 0.9573664137130466\n",
      "Iteration 179: Loss = 0.9549175064434332\n",
      "Iteration 180: Loss = 0.9524808863179628\n",
      "Iteration 181: Loss = 0.9500564916871364\n",
      "Iteration 182: Loss = 0.9476442612107722\n",
      "Iteration 183: Loss = 0.9452441338564569\n",
      "Iteration 184: Loss = 0.9428560488980022\n",
      "Iteration 185: Loss = 0.940479945913906\n",
      "Iteration 186: Loss = 0.9381157647858273\n",
      "Iteration 187: Loss = 0.9357634456970598\n",
      "Iteration 188: Loss = 0.9334229291310241\n",
      "Iteration 189: Loss = 0.931094155869757\n",
      "Iteration 190: Loss = 0.9287770669924168\n",
      "Iteration 191: Loss = 0.9264716038737915\n",
      "Iteration 192: Loss = 0.9241777081828131\n",
      "Iteration 193: Loss = 0.9218953218810864\n",
      "Iteration 194: Loss = 0.919624387221416\n",
      "Iteration 195: Loss = 0.9173648467463491\n",
      "Iteration 196: Loss = 0.915116643286717\n",
      "Iteration 197: Loss = 0.9128797199601952\n",
      "Iteration 198: Loss = 0.9106540201698566\n",
      "Iteration 199: Loss = 0.9084394876027467\n",
      "Iteration 200: Loss = 0.9062360662284533\n",
      "Iteration 201: Loss = 0.904043700297693\n",
      "Iteration 202: Loss = 0.9018623343408969\n",
      "Iteration 203: Loss = 0.8996919131668104\n",
      "Iteration 204: Loss = 0.8975323818610952\n",
      "Iteration 205: Loss = 0.8953836857849398\n",
      "Iteration 206: Loss = 0.8932457705736774\n",
      "Iteration 207: Loss = 0.8911185821354105\n",
      "Iteration 208: Loss = 0.8890020666496424\n",
      "Iteration 209: Loss = 0.8868961705659153\n",
      "Iteration 210: Loss = 0.8848008406024541\n",
      "Iteration 211: Loss = 0.8827160237448226\n",
      "Iteration 212: Loss = 0.8806416672445755\n",
      "Iteration 213: Loss = 0.8785777186179301\n",
      "Iteration 214: Loss = 0.8765241256444354\n",
      "Iteration 215: Loss = 0.8744808363656507\n",
      "Iteration 216: Loss = 0.8724477990838303\n",
      "Iteration 217: Loss = 0.8704249623606195\n",
      "Iteration 218: Loss = 0.8684122750157487\n",
      "Iteration 219: Loss = 0.8664096861257392\n",
      "Iteration 220: Loss = 0.8644171450226174\n",
      "Iteration 221: Loss = 0.8624346012926305\n",
      "Iteration 222: Loss = 0.8604620047749715\n",
      "Iteration 223: Loss = 0.8584993055605102\n",
      "Iteration 224: Loss = 0.85654645399053\n",
      "Iteration 225: Loss = 0.8546034006554728\n",
      "Iteration 226: Loss = 0.852670096393688\n",
      "Iteration 227: Loss = 0.8507464922901864\n",
      "Iteration 228: Loss = 0.8488325396754086\n",
      "Iteration 229: Loss = 0.8469281901239853\n",
      "Iteration 230: Loss = 0.845033395453521\n",
      "Iteration 231: Loss = 0.8431481077233673\n",
      "Iteration 232: Loss = 0.8412722792334153\n",
      "Iteration 233: Loss = 0.8394058625228844\n",
      "Iteration 234: Loss = 0.8375488103691254\n",
      "Iteration 235: Loss = 0.8357010757864224\n",
      "Iteration 236: Loss = 0.8338626120248078\n",
      "Iteration 237: Loss = 0.8320333725688753\n",
      "Iteration 238: Loss = 0.8302133111366061\n",
      "Iteration 239: Loss = 0.8284023816781959\n",
      "Iteration 240: Loss = 0.8266005383748907\n",
      "Iteration 241: Loss = 0.824807735637828\n",
      "Iteration 242: Loss = 0.8230239281068823\n",
      "Iteration 243: Loss = 0.8212490706495181\n",
      "Iteration 244: Loss = 0.8194831183596482\n",
      "Iteration 245: Loss = 0.817726026556497\n",
      "Iteration 246: Loss = 0.8159777507834696\n",
      "Iteration 247: Loss = 0.8142382468070279\n",
      "Iteration 248: Loss = 0.8125074706155712\n",
      "Iteration 249: Loss = 0.810785378418323\n",
      "Iteration 250: Loss = 0.8090719266442208\n",
      "Iteration 251: Loss = 0.8073670719408181\n",
      "Iteration 252: Loss = 0.8056707711731829\n",
      "Iteration 253: Loss = 0.8039829814228094\n",
      "Iteration 254: Loss = 0.8023036599865301\n",
      "Iteration 255: Loss = 0.8006327643754374\n",
      "Iteration 256: Loss = 0.7989702523138068\n",
      "Iteration 257: Loss = 0.7973160817380287\n",
      "Iteration 258: Loss = 0.7956702107955413\n",
      "Iteration 259: Loss = 0.794032597843777\n",
      "Iteration 260: Loss = 0.7924032014491031\n",
      "Iteration 261: Loss = 0.7907819803857777\n",
      "Iteration 262: Loss = 0.7891688936349026\n",
      "Iteration 263: Loss = 0.7875639003833887\n",
      "Iteration 264: Loss = 0.785966960022925\n",
      "Iteration 265: Loss = 0.7843780321489455\n",
      "Iteration 266: Loss = 0.7827970765596103\n",
      "Iteration 267: Loss = 0.7812240532547894\n",
      "Iteration 268: Loss = 0.7796589224350486\n",
      "Iteration 269: Loss = 0.7781016445006438\n",
      "Iteration 270: Loss = 0.7765521800505182\n",
      "Iteration 271: Loss = 0.775010489881305\n",
      "Iteration 272: Loss = 0.7734765349863388\n",
      "Iteration 273: Loss = 0.7719502765546645\n",
      "Iteration 274: Loss = 0.7704316759700561\n",
      "Iteration 275: Loss = 0.7689206948100424\n",
      "Iteration 276: Loss = 0.7674172948449332\n",
      "Iteration 277: Loss = 0.7659214380368494\n",
      "Iteration 278: Loss = 0.7644330865387656\n",
      "Iteration 279: Loss = 0.7629522026935496\n",
      "Iteration 280: Loss = 0.7614787490330089\n",
      "Iteration 281: Loss = 0.7600126882769446\n",
      "Iteration 282: Loss = 0.7585539833322085\n",
      "Iteration 283: Loss = 0.7571025972917607\n",
      "Iteration 284: Loss = 0.7556584934337419\n",
      "Iteration 285: Loss = 0.7542216352205383\n",
      "Iteration 286: Loss = 0.7527919862978621\n",
      "Iteration 287: Loss = 0.7513695104938276\n",
      "Iteration 288: Loss = 0.7499541718180389\n",
      "Iteration 289: Loss = 0.7485459344606786\n",
      "Iteration 290: Loss = 0.7471447627916004\n",
      "Iteration 291: Loss = 0.7457506213594285\n",
      "Iteration 292: Loss = 0.7443634748906622\n",
      "Iteration 293: Loss = 0.742983288288781\n",
      "Iteration 294: Loss = 0.7416100266333578\n",
      "Iteration 295: Loss = 0.7402436551791753\n",
      "Iteration 296: Loss = 0.7388841393553474\n",
      "Iteration 297: Loss = 0.7375314447644421\n",
      "Iteration 298: Loss = 0.7361855371816145\n",
      "Iteration 299: Loss = 0.7348463825537401\n",
      "Iteration 300: Loss = 0.733513946998551\n",
      "Iteration 301: Loss = 0.7321881968037802\n",
      "Iteration 302: Loss = 0.7308690984263093\n",
      "Iteration 303: Loss = 0.7295566184913193\n",
      "Iteration 304: Loss = 0.7282507237914446\n",
      "Iteration 305: Loss = 0.7269513812859361\n",
      "Iteration 306: Loss = 0.7256585580998212\n",
      "Iteration 307: Loss = 0.7243722215230767\n",
      "Iteration 308: Loss = 0.7230923390097956\n",
      "Iteration 309: Loss = 0.7218188781773696\n",
      "Iteration 310: Loss = 0.7205518068056656\n",
      "Iteration 311: Loss = 0.7192910928362101\n",
      "Iteration 312: Loss = 0.7180367043713832\n",
      "Iteration 313: Loss = 0.7167886096736052\n",
      "Iteration 314: Loss = 0.7155467771645374\n",
      "Iteration 315: Loss = 0.7143111754242831\n",
      "Iteration 316: Loss = 0.7130817731905901\n",
      "Iteration 317: Loss = 0.7118585393580628\n",
      "Iteration 318: Loss = 0.7106414429773737\n",
      "Iteration 319: Loss = 0.709430453254479\n",
      "Iteration 320: Loss = 0.7082255395498418\n",
      "Iteration 321: Loss = 0.707026671377657\n",
      "Iteration 322: Loss = 0.7058338184050766\n",
      "Iteration 323: Loss = 0.7046469504514473\n",
      "Iteration 324: Loss = 0.7034660374875416\n",
      "Iteration 325: Loss = 0.7022910496348029\n",
      "Iteration 326: Loss = 0.7011219571645859\n",
      "Iteration 327: Loss = 0.6999587304974055\n",
      "Iteration 328: Loss = 0.6988013402021893\n",
      "Iteration 329: Loss = 0.6976497569955327\n",
      "Iteration 330: Loss = 0.696503951740957\n",
      "Iteration 331: Loss = 0.6953638954481718\n",
      "Iteration 332: Loss = 0.6942295592723453\n",
      "Iteration 333: Loss = 0.6931009145133702\n",
      "Iteration 334: Loss = 0.6919779326151372\n",
      "Iteration 335: Loss = 0.6908605851648179\n",
      "Iteration 336: Loss = 0.6897488438921399\n",
      "Iteration 337: Loss = 0.688642680668675\n",
      "Iteration 338: Loss = 0.6875420675071259\n",
      "Iteration 339: Loss = 0.6864469765606199\n",
      "Iteration 340: Loss = 0.685357380122003\n",
      "Iteration 341: Loss = 0.6842732506231366\n",
      "Iteration 342: Loss = 0.6831945606342053\n",
      "Iteration 343: Loss = 0.6821212828630199\n",
      "Iteration 344: Loss = 0.6810533901543249\n",
      "Iteration 345: Loss = 0.6799908554891148\n",
      "Iteration 346: Loss = 0.6789336519839497\n",
      "Iteration 347: Loss = 0.6778817528902743\n",
      "Iteration 348: Loss = 0.6768351315937406\n",
      "Iteration 349: Loss = 0.6757937616135372\n",
      "Iteration 350: Loss = 0.6747576166017156\n",
      "Iteration 351: Loss = 0.6737266703425256\n",
      "Iteration 352: Loss = 0.6727008967517537\n",
      "Iteration 353: Loss = 0.6716802698760596\n",
      "Iteration 354: Loss = 0.6706647638923222\n",
      "Iteration 355: Loss = 0.6696543531069845\n",
      "Iteration 356: Loss = 0.6686490119554043\n",
      "Iteration 357: Loss = 0.6676487150012085\n",
      "Iteration 358: Loss = 0.6666534369356472\n",
      "Iteration 359: Loss = 0.6656631525769559\n",
      "Iteration 360: Loss = 0.6646778368697154\n",
      "Iteration 361: Loss = 0.6636974648842203\n",
      "Iteration 362: Loss = 0.6627220118158483\n",
      "Iteration 363: Loss = 0.6617514529844302\n",
      "Iteration 364: Loss = 0.6607857638336286\n",
      "Iteration 365: Loss = 0.6598249199303146\n",
      "Iteration 366: Loss = 0.6588688969639489\n",
      "Iteration 367: Loss = 0.6579176707459689\n",
      "Iteration 368: Loss = 0.6569712172091757\n",
      "Iteration 369: Loss = 0.6560295124071244\n",
      "Iteration 370: Loss = 0.6550925325135205\n",
      "Iteration 371: Loss = 0.6541602538216148\n",
      "Iteration 372: Loss = 0.6532326527436023\n",
      "Iteration 373: Loss = 0.6523097058100302\n",
      "Iteration 374: Loss = 0.6513913896691997\n",
      "Iteration 375: Loss = 0.6504776810865783\n",
      "Iteration 376: Loss = 0.6495685569442089\n",
      "Iteration 377: Loss = 0.6486639942401269\n",
      "Iteration 378: Loss = 0.6477639700877781\n",
      "Iteration 379: Loss = 0.646868461715438\n",
      "Iteration 380: Loss = 0.6459774464656389\n",
      "Iteration 381: Loss = 0.6450909017945935\n",
      "Iteration 382: Loss = 0.6442088052716253\n",
      "Iteration 383: Loss = 0.6433311345786028\n",
      "Iteration 384: Loss = 0.6424578675093708\n",
      "Iteration 385: Loss = 0.641588981969195\n",
      "Iteration 386: Loss = 0.6407244559741964\n",
      "Iteration 387: Loss = 0.6398642676507986\n",
      "Iteration 388: Loss = 0.6390083952351737\n",
      "Iteration 389: Loss = 0.6381568170726919\n",
      "Iteration 390: Loss = 0.6373095116173734\n",
      "Iteration 391: Loss = 0.6364664574313417\n",
      "Iteration 392: Loss = 0.6356276331842861\n",
      "Iteration 393: Loss = 0.6347930176529135\n",
      "Iteration 394: Loss = 0.6339625897204207\n",
      "Iteration 395: Loss = 0.6331363283759551\n",
      "Iteration 396: Loss = 0.6323142127140824\n",
      "Iteration 397: Loss = 0.6314962219342601\n",
      "Iteration 398: Loss = 0.6306823353403115\n",
      "Iteration 399: Loss = 0.6298725323398982\n",
      "Iteration 400: Loss = 0.6290667924440049\n",
      "Iteration 401: Loss = 0.6282650952664148\n",
      "Iteration 402: Loss = 0.6274674205231993\n",
      "Iteration 403: Loss = 0.6266737480322009\n",
      "Iteration 404: Loss = 0.6258840577125249\n",
      "Iteration 405: Loss = 0.6250983295840303\n",
      "Iteration 406: Loss = 0.6243165437668239\n",
      "Iteration 407: Loss = 0.6235386804807578\n",
      "Iteration 408: Loss = 0.6227647200449303\n",
      "Iteration 409: Loss = 0.6219946428771848\n",
      "Iteration 410: Loss = 0.6212284294936173\n",
      "Iteration 411: Loss = 0.6204660605080826\n",
      "Iteration 412: Loss = 0.6197075166317023\n",
      "Iteration 413: Loss = 0.6189527786723796\n",
      "Iteration 414: Loss = 0.6182018275343095\n",
      "Iteration 415: Loss = 0.6174546442175023\n",
      "Iteration 416: Loss = 0.6167112098172948\n",
      "Iteration 417: Loss = 0.6159715055238785\n",
      "Iteration 418: Loss = 0.6152355126218207\n",
      "Iteration 419: Loss = 0.6145032124895904\n",
      "Iteration 420: Loss = 0.6137745865990892\n",
      "Iteration 421: Loss = 0.6130496165151818\n",
      "Iteration 422: Loss = 0.6123282838952286\n",
      "Iteration 423: Loss = 0.6116105704886242\n",
      "Iteration 424: Loss = 0.6108964581363296\n",
      "Iteration 425: Loss = 0.6101859287704215\n",
      "Iteration 426: Loss = 0.6094789644136286\n",
      "Iteration 427: Loss = 0.6087755471788777\n",
      "Iteration 428: Loss = 0.6080756592688442\n",
      "Iteration 429: Loss = 0.6073792829754991\n",
      "Iteration 430: Loss = 0.6066864006796605\n",
      "Iteration 431: Loss = 0.6059969948505506\n",
      "Iteration 432: Loss = 0.6053110480453489\n",
      "Iteration 433: Loss = 0.6046285429087541\n",
      "Iteration 434: Loss = 0.6039494621725429\n",
      "Iteration 435: Loss = 0.6032737886551323\n",
      "Iteration 436: Loss = 0.6026015052611475\n",
      "Iteration 437: Loss = 0.6019325949809882\n",
      "Iteration 438: Loss = 0.6012670408903967\n",
      "Iteration 439: Loss = 0.6006048261500332\n",
      "Iteration 440: Loss = 0.5999459340050446\n",
      "Iteration 441: Loss = 0.5992903477846453\n",
      "Iteration 442: Loss = 0.5986380509016945\n",
      "Iteration 443: Loss = 0.5979890268522746\n",
      "Iteration 444: Loss = 0.5973432592152746\n",
      "Iteration 445: Loss = 0.5967007316519748\n",
      "Iteration 446: Loss = 0.596061427905634\n",
      "Iteration 447: Loss = 0.5954253318010772\n",
      "Iteration 448: Loss = 0.594792427244287\n",
      "Iteration 449: Loss = 0.5941626982219951\n",
      "Iteration 450: Loss = 0.5935361288012797\n",
      "Iteration 451: Loss = 0.5929127031291587\n",
      "Iteration 452: Loss = 0.592292405432193\n",
      "Iteration 453: Loss = 0.5916752200160842\n",
      "Iteration 454: Loss = 0.5910611312652777\n",
      "Iteration 455: Loss = 0.5904501236425698\n",
      "Iteration 456: Loss = 0.589842181688712\n",
      "Iteration 457: Loss = 0.5892372900220209\n",
      "Iteration 458: Loss = 0.5886354333379907\n",
      "Iteration 459: Loss = 0.5880365964089024\n",
      "Iteration 460: Loss = 0.5874407640834423\n",
      "Iteration 461: Loss = 0.5868479212863154\n",
      "Iteration 462: Loss = 0.5862580530178664\n",
      "Iteration 463: Loss = 0.5856711443536993\n",
      "Iteration 464: Loss = 0.5850871804442989\n",
      "Iteration 465: Loss = 0.5845061465146565\n",
      "Iteration 466: Loss = 0.583928027863896\n",
      "Iteration 467: Loss = 0.5833528098649003\n",
      "Iteration 468: Loss = 0.5827804779639435\n",
      "Iteration 469: Loss = 0.5822110176803214\n",
      "Iteration 470: Loss = 0.5816444146059835\n",
      "Iteration 471: Loss = 0.5810806544051731\n",
      "Iteration 472: Loss = 0.5805197228140597\n",
      "Iteration 473: Loss = 0.5799616056403805\n",
      "Iteration 474: Loss = 0.5794062887630816\n",
      "Iteration 475: Loss = 0.5788537581319593\n",
      "Iteration 476: Loss = 0.5783039997673056\n",
      "Iteration 477: Loss = 0.5777569997595545\n",
      "Iteration 478: Loss = 0.5772127442689297\n",
      "Iteration 479: Loss = 0.5766712195250944\n",
      "Iteration 480: Loss = 0.5761324118268032\n",
      "Iteration 481: Loss = 0.5755963075415557\n",
      "Iteration 482: Loss = 0.5750628931052502\n",
      "Iteration 483: Loss = 0.5745321550218428\n",
      "Iteration 484: Loss = 0.5740040798630032\n",
      "Iteration 485: Loss = 0.5734786542677777\n",
      "Iteration 486: Loss = 0.5729558649422493\n",
      "Iteration 487: Loss = 0.5724356986592015\n",
      "Iteration 488: Loss = 0.5719181422577847\n",
      "Iteration 489: Loss = 0.5714031826431821\n",
      "Iteration 490: Loss = 0.5708908067862789\n",
      "Iteration 491: Loss = 0.570381001723332\n",
      "Iteration 492: Loss = 0.5698737545556444\n",
      "Iteration 493: Loss = 0.5693690524492345\n",
      "Iteration 494: Loss = 0.5688668826345159\n",
      "Iteration 495: Loss = 0.5683672324059715\n",
      "Iteration 496: Loss = 0.5678700891218333\n",
      "Iteration 497: Loss = 0.5673754402037609\n",
      "Iteration 498: Loss = 0.5668832731365255\n",
      "Iteration 499: Loss = 0.5663935754676922\n",
      "Iteration 500: Loss = 0.565906334807304\n",
      "Iteration 501: Loss = 0.5654215388275706\n",
      "Iteration 502: Loss = 0.5649391752625549\n",
      "Iteration 503: Loss = 0.564459231907861\n",
      "Iteration 504: Loss = 0.5639816966203299\n",
      "Iteration 505: Loss = 0.5635065573177286\n",
      "Iteration 506: Loss = 0.5630338019784447\n",
      "Iteration 507: Loss = 0.5625634186411846\n",
      "Iteration 508: Loss = 0.5620953954046678\n",
      "Iteration 509: Loss = 0.561629720427328\n",
      "Iteration 510: Loss = 0.5611663819270133\n",
      "Iteration 511: Loss = 0.5607053681806858\n",
      "Iteration 512: Loss = 0.5602466675241285\n",
      "Iteration 513: Loss = 0.5597902683516486\n",
      "Iteration 514: Loss = 0.5593361591157816\n",
      "Iteration 515: Loss = 0.5588843283270023\n",
      "Iteration 516: Loss = 0.5584347645534344\n",
      "Iteration 517: Loss = 0.5579874564205585\n",
      "Iteration 518: Loss = 0.5575423926109256\n",
      "Iteration 519: Loss = 0.5570995618638717\n",
      "Iteration 520: Loss = 0.5566589529752304\n",
      "Iteration 521: Loss = 0.5562205547970532\n",
      "Iteration 522: Loss = 0.555784356237324\n",
      "Iteration 523: Loss = 0.5553503462596804\n",
      "Iteration 524: Loss = 0.5549185138831326\n",
      "Iteration 525: Loss = 0.5544888481817887\n",
      "Iteration 526: Loss = 0.554061338284575\n",
      "Iteration 527: Loss = 0.5536359733749624\n",
      "Iteration 528: Loss = 0.5532127426906929\n",
      "Iteration 529: Loss = 0.5527916355235073\n",
      "Iteration 530: Loss = 0.552372641218874\n",
      "Iteration 531: Loss = 0.5519557491757188\n",
      "Iteration 532: Loss = 0.5515409488461572\n",
      "Iteration 533: Loss = 0.5511282297352291\n",
      "Iteration 534: Loss = 0.5507175814006304\n",
      "Iteration 535: Loss = 0.5503089934524509\n",
      "Iteration 536: Loss = 0.5499024555529103\n",
      "Iteration 537: Loss = 0.5494979574160971\n",
      "Iteration 538: Loss = 0.5490954888077084\n",
      "Iteration 539: Loss = 0.548695039544791\n",
      "Iteration 540: Loss = 0.5482965994954843\n",
      "Iteration 541: Loss = 0.5479001585787613\n",
      "Iteration 542: Loss = 0.5475057067641772\n",
      "Iteration 543: Loss = 0.5471132340716139\n",
      "Iteration 544: Loss = 0.5467227305710267\n",
      "Iteration 545: Loss = 0.5463341863821948\n",
      "Iteration 546: Loss = 0.5459475916744696\n",
      "Iteration 547: Loss = 0.5455629366665272\n",
      "Iteration 548: Loss = 0.5451802116261207\n",
      "Iteration 549: Loss = 0.5447994068698334\n",
      "Iteration 550: Loss = 0.5444205127628341\n",
      "Iteration 551: Loss = 0.5440435197186336\n",
      "Iteration 552: Loss = 0.5436684181988418\n",
      "Iteration 553: Loss = 0.5432951987129265\n",
      "Iteration 554: Loss = 0.5429238518179739\n",
      "Iteration 555: Loss = 0.542554368118448\n",
      "Iteration 556: Loss = 0.542186738265955\n",
      "Iteration 557: Loss = 0.541820952959005\n",
      "Iteration 558: Loss = 0.541457002942777\n",
      "Iteration 559: Loss = 0.541094879008887\n",
      "Iteration 560: Loss = 0.5407345719951503\n",
      "Iteration 561: Loss = 0.5403760727853549\n",
      "Iteration 562: Loss = 0.5400193723090284\n",
      "Iteration 563: Loss = 0.5396644615412064\n",
      "Iteration 564: Loss = 0.539311331502209\n",
      "Iteration 565: Loss = 0.5389599732574087\n",
      "Iteration 566: Loss = 0.5386103779170079\n",
      "Iteration 567: Loss = 0.5382625366358136\n",
      "Iteration 568: Loss = 0.5379164406130093\n",
      "Iteration 569: Loss = 0.5375720810919395\n",
      "Iteration 570: Loss = 0.5372294493598815\n",
      "Iteration 571: Loss = 0.53688853674783\n",
      "Iteration 572: Loss = 0.5365493346302744\n",
      "Iteration 573: Loss = 0.5362118344249804\n",
      "Iteration 574: Loss = 0.5358760275927777\n",
      "Iteration 575: Loss = 0.535541905637337\n",
      "Iteration 576: Loss = 0.5352094601049597\n",
      "Iteration 577: Loss = 0.5348786825843648\n",
      "Iteration 578: Loss = 0.5345495647064692\n",
      "Iteration 579: Loss = 0.5342220981441864\n",
      "Iteration 580: Loss = 0.5338962746122061\n",
      "Iteration 581: Loss = 0.5335720858667908\n",
      "Iteration 582: Loss = 0.5332495237055647\n",
      "Iteration 583: Loss = 0.5329285799673055\n",
      "Iteration 584: Loss = 0.5326092465317406\n",
      "Iteration 585: Loss = 0.5322915153193399\n",
      "Iteration 586: Loss = 0.5319753782911095\n",
      "Iteration 587: Loss = 0.5316608274483932\n",
      "Iteration 588: Loss = 0.5313478548326648\n",
      "Iteration 589: Loss = 0.5310364525253308\n",
      "Iteration 590: Loss = 0.5307266126475283\n",
      "Iteration 591: Loss = 0.5304183273599252\n",
      "Iteration 592: Loss = 0.5301115888625221\n",
      "Iteration 593: Loss = 0.529806389394456\n",
      "Iteration 594: Loss = 0.5295027212338038\n",
      "Iteration 595: Loss = 0.5292005766973847\n",
      "Iteration 596: Loss = 0.5288999481405694\n",
      "Iteration 597: Loss = 0.5286008279570835\n",
      "Iteration 598: Loss = 0.5283032085788162\n",
      "Iteration 599: Loss = 0.5280070824756298\n",
      "Iteration 600: Loss = 0.5277124421551679\n",
      "Iteration 601: Loss = 0.5274192801626656\n",
      "Iteration 602: Loss = 0.5271275890807628\n",
      "Iteration 603: Loss = 0.526837361529314\n",
      "Iteration 604: Loss = 0.5265485901652041\n",
      "Iteration 605: Loss = 0.5262612676821601\n",
      "Iteration 606: Loss = 0.5259753868105681\n",
      "Iteration 607: Loss = 0.5256909403172889\n",
      "Iteration 608: Loss = 0.5254079210054744\n",
      "Iteration 609: Loss = 0.5251263217143863\n",
      "Iteration 610: Loss = 0.5248461353192145\n",
      "Iteration 611: Loss = 0.5245673547308967\n",
      "Iteration 612: Loss = 0.5242899728959393\n",
      "Iteration 613: Loss = 0.5240139827962395\n",
      "Iteration 614: Loss = 0.5237393774489062\n",
      "Iteration 615: Loss = 0.5234661499060851\n",
      "Iteration 616: Loss = 0.523194293254781\n",
      "Iteration 617: Loss = 0.5229238006166853\n",
      "Iteration 618: Loss = 0.5226546651479993\n",
      "Iteration 619: Loss = 0.522386880039263\n",
      "Iteration 620: Loss = 0.522120438515183\n",
      "Iteration 621: Loss = 0.5218553338344584\n",
      "Iteration 622: Loss = 0.5215915592896138\n",
      "Iteration 623: Loss = 0.5213291082068267\n",
      "Iteration 624: Loss = 0.5210679739457599\n",
      "Iteration 625: Loss = 0.520808149899395\n",
      "Iteration 626: Loss = 0.5205496294938617\n",
      "Iteration 627: Loss = 0.5202924061882744\n",
      "Iteration 628: Loss = 0.5200364734745652\n",
      "Iteration 629: Loss = 0.5197818248773203\n",
      "Iteration 630: Loss = 0.5195284539536155\n",
      "Iteration 631: Loss = 0.5192763542928531\n",
      "Iteration 632: Loss = 0.5190255195166\n",
      "Iteration 633: Loss = 0.5187759432784269\n",
      "Iteration 634: Loss = 0.5185276192637462\n",
      "Iteration 635: Loss = 0.5182805411896526\n",
      "Iteration 636: Loss = 0.5180347028047662\n",
      "Iteration 637: Loss = 0.517790097889072\n",
      "Iteration 638: Loss = 0.5175467202537629\n",
      "Iteration 639: Loss = 0.517304563741085\n",
      "Iteration 640: Loss = 0.5170636222241772\n",
      "Iteration 641: Loss = 0.5168238896069226\n",
      "Iteration 642: Loss = 0.5165853598237898\n",
      "Iteration 643: Loss = 0.5163480268396802\n",
      "Iteration 644: Loss = 0.5161118846497753\n",
      "Iteration 645: Loss = 0.515876927279387\n",
      "Iteration 646: Loss = 0.5156431487838026\n",
      "Iteration 647: Loss = 0.5154105432481371\n",
      "Iteration 648: Loss = 0.5151791047871831\n",
      "Iteration 649: Loss = 0.5149488275452621\n",
      "Iteration 650: Loss = 0.5147197056960747\n",
      "Iteration 651: Loss = 0.5144917334425546\n",
      "Iteration 652: Loss = 0.5142649050167222\n",
      "Iteration 653: Loss = 0.514039214679538\n",
      "Iteration 654: Loss = 0.5138146567207575\n",
      "Iteration 655: Loss = 0.5135912254587874\n",
      "Iteration 656: Loss = 0.5133689152405392\n",
      "Iteration 657: Loss = 0.5131477204412912\n",
      "Iteration 658: Loss = 0.5129276354645406\n",
      "Iteration 659: Loss = 0.5127086547418652\n",
      "Iteration 660: Loss = 0.5124907727327823\n",
      "Iteration 661: Loss = 0.5122739839246083\n",
      "Iteration 662: Loss = 0.5120582828323178\n",
      "Iteration 663: Loss = 0.5118436639984071\n",
      "Iteration 664: Loss = 0.5116301219927539\n",
      "Iteration 665: Loss = 0.5114176514124813\n",
      "Iteration 666: Loss = 0.5112062468818213\n",
      "Iteration 667: Loss = 0.5109959030519785\n",
      "Iteration 668: Loss = 0.5107866146009931\n",
      "Iteration 669: Loss = 0.5105783762336096\n",
      "Iteration 670: Loss = 0.5103711826811382\n",
      "Iteration 671: Loss = 0.510165028701328\n",
      "Iteration 672: Loss = 0.5099599090782275\n",
      "Iteration 673: Loss = 0.5097558186220559\n",
      "Iteration 674: Loss = 0.5095527521690736\n",
      "Iteration 675: Loss = 0.5093507045814482\n",
      "Iteration 676: Loss = 0.5091496707471259\n",
      "Iteration 677: Loss = 0.5089496455797012\n",
      "Iteration 678: Loss = 0.5087506240182914\n",
      "Iteration 679: Loss = 0.5085526010274044\n",
      "Iteration 680: Loss = 0.508355571596814\n",
      "Iteration 681: Loss = 0.508159530741432\n",
      "Iteration 682: Loss = 0.5079644735011825\n",
      "Iteration 683: Loss = 0.507770394940876\n",
      "Iteration 684: Loss = 0.507577290150085\n",
      "Iteration 685: Loss = 0.5073851542430194\n",
      "Iteration 686: Loss = 0.5071939823584038\n",
      "Iteration 687: Loss = 0.507003769659353\n",
      "Iteration 688: Loss = 0.5068145113332512\n",
      "Iteration 689: Loss = 0.5066262025916275\n",
      "Iteration 690: Loss = 0.5064388386700388\n",
      "Iteration 691: Loss = 0.5062524148279463\n",
      "Iteration 692: Loss = 0.5060669263485961\n",
      "Iteration 693: Loss = 0.5058823685389007\n",
      "Iteration 694: Loss = 0.5056987367293192\n",
      "Iteration 695: Loss = 0.5055160262737396\n",
      "Iteration 696: Loss = 0.505334232549362\n",
      "Iteration 697: Loss = 0.5051533509565803\n",
      "Iteration 698: Loss = 0.5049733769188662\n",
      "Iteration 699: Loss = 0.5047943058826546\n",
      "Iteration 700: Loss = 0.5046161333172273\n",
      "Iteration 701: Loss = 0.5044388547145967\n",
      "Iteration 702: Loss = 0.5042624655893967\n",
      "Iteration 703: Loss = 0.5040869614787636\n",
      "Iteration 704: Loss = 0.5039123379422258\n",
      "Iteration 705: Loss = 0.5037385905615933\n",
      "Iteration 706: Loss = 0.5035657149408422\n",
      "Iteration 707: Loss = 0.5033937067060051\n",
      "Iteration 708: Loss = 0.5032225615050616\n",
      "Iteration 709: Loss = 0.5030522750078262\n",
      "Iteration 710: Loss = 0.5028828429058401\n",
      "Iteration 711: Loss = 0.5027142609122616\n",
      "Iteration 712: Loss = 0.5025465247617585\n",
      "Iteration 713: Loss = 0.5023796302103983\n",
      "Iteration 714: Loss = 0.5022135730355421\n",
      "Iteration 715: Loss = 0.5020483490357396\n",
      "Iteration 716: Loss = 0.5018839540306181\n",
      "Iteration 717: Loss = 0.5017203838607813\n",
      "Iteration 718: Loss = 0.501557634387701\n",
      "Iteration 719: Loss = 0.5013957014936147\n",
      "Iteration 720: Loss = 0.5012345810814206\n",
      "Iteration 721: Loss = 0.5010742690745729\n",
      "Iteration 722: Loss = 0.5009147614169794\n",
      "Iteration 723: Loss = 0.5007560540728987\n",
      "Iteration 724: Loss = 0.5005981430268401\n",
      "Iteration 725: Loss = 0.5004410242834582\n",
      "Iteration 726: Loss = 0.5002846938674543\n",
      "Iteration 727: Loss = 0.5001291478234762\n",
      "Iteration 728: Loss = 0.49997438221601664\n",
      "Iteration 729: Loss = 0.49982039312931437\n",
      "Iteration 730: Loss = 0.4996671766672549\n",
      "Iteration 731: Loss = 0.49951472895327226\n",
      "Iteration 732: Loss = 0.4993630461302516\n",
      "Iteration 733: Loss = 0.49921212436042933\n",
      "Iteration 734: Loss = 0.49906195982529805\n",
      "Iteration 735: Loss = 0.49891254872551\n",
      "Iteration 736: Loss = 0.49876388728077947\n",
      "Iteration 737: Loss = 0.4986159717297885\n",
      "Iteration 738: Loss = 0.49846879833008934\n",
      "Iteration 739: Loss = 0.4983223633580143\n",
      "Iteration 740: Loss = 0.4981766631085778\n",
      "Iteration 741: Loss = 0.4980316938953829\n",
      "Iteration 742: Loss = 0.49788745205052937\n",
      "Iteration 743: Loss = 0.4977439339245205\n",
      "Iteration 744: Loss = 0.4976011358861694\n",
      "Iteration 745: Loss = 0.49745905432251014\n",
      "Iteration 746: Loss = 0.4973176856387035\n",
      "Iteration 747: Loss = 0.4971770262579467\n",
      "Iteration 748: Loss = 0.4970370726213833\n",
      "Iteration 749: Loss = 0.49689782118801445\n",
      "Iteration 750: Loss = 0.49675926843460544\n",
      "Iteration 751: Loss = 0.49662141085560124\n",
      "Iteration 752: Loss = 0.49648424496303384\n",
      "Iteration 753: Loss = 0.4963477672864369\n",
      "Iteration 754: Loss = 0.49621197437275727\n",
      "Iteration 755: Loss = 0.49607686278626534\n",
      "Iteration 756: Loss = 0.49594242910847186\n",
      "Iteration 757: Loss = 0.4958086699380381\n",
      "Iteration 758: Loss = 0.4956755818906922\n",
      "Iteration 759: Loss = 0.4955431615991408\n",
      "Iteration 760: Loss = 0.49541140571298836\n",
      "Iteration 761: Loss = 0.4952803108986471\n",
      "Iteration 762: Loss = 0.495149873839257\n",
      "Iteration 763: Loss = 0.4950200912345981\n",
      "Iteration 764: Loss = 0.49489095980101094\n",
      "Iteration 765: Loss = 0.49476247627131037\n",
      "Iteration 766: Loss = 0.49463463739470453\n",
      "Iteration 767: Loss = 0.4945074399367113\n",
      "Iteration 768: Loss = 0.49438088067907854\n",
      "Iteration 769: Loss = 0.4942549564196999\n",
      "Iteration 770: Loss = 0.49412966397253594\n",
      "Iteration 771: Loss = 0.4940050001675337\n",
      "Iteration 772: Loss = 0.4938809618505441\n",
      "Iteration 773: Loss = 0.4937575458832442\n",
      "Iteration 774: Loss = 0.49363474914305794\n",
      "Iteration 775: Loss = 0.49351256852307596\n",
      "Iteration 776: Loss = 0.4933910009319779\n",
      "Iteration 777: Loss = 0.4932700432939523\n",
      "Iteration 778: Loss = 0.49314969254862256\n",
      "Iteration 779: Loss = 0.4930299456509657\n",
      "Iteration 780: Loss = 0.4929107995712372\n",
      "Iteration 781: Loss = 0.49279225129489374\n",
      "Iteration 782: Loss = 0.4926742978225173\n",
      "Iteration 783: Loss = 0.4925569361697397\n",
      "Iteration 784: Loss = 0.4924401633671658\n",
      "Iteration 785: Loss = 0.4923239764602998\n",
      "Iteration 786: Loss = 0.4922083725094685\n",
      "Iteration 787: Loss = 0.4920933485897493\n",
      "Iteration 788: Loss = 0.4919789017908949\n",
      "Iteration 789: Loss = 0.49186502921726083\n",
      "Iteration 790: Loss = 0.4917517279877286\n",
      "Iteration 791: Loss = 0.49163899523563803\n",
      "Iteration 792: Loss = 0.4915268281087114\n",
      "Iteration 793: Loss = 0.4914152237689807\n",
      "Iteration 794: Loss = 0.49130417939271925\n",
      "Iteration 795: Loss = 0.4911936921703659\n",
      "Iteration 796: Loss = 0.4910837593064583\n",
      "Iteration 797: Loss = 0.4909743780195589\n",
      "Iteration 798: Loss = 0.4908655455421859\n",
      "Iteration 799: Loss = 0.4907572591207428\n",
      "Iteration 800: Loss = 0.4906495160154505\n",
      "Iteration 801: Loss = 0.49054231350027355\n",
      "Iteration 802: Loss = 0.4904356488628565\n",
      "Iteration 803: Loss = 0.4903295194044528\n",
      "Iteration 804: Loss = 0.4902239224398546\n",
      "Iteration 805: Loss = 0.4901188552973299\n",
      "Iteration 806: Loss = 0.49001431531854867\n",
      "Iteration 807: Loss = 0.48991029985852097\n",
      "Iteration 808: Loss = 0.4898068062855262\n",
      "Iteration 809: Loss = 0.48970383198105066\n",
      "Iteration 810: Loss = 0.4896013743397162\n",
      "Iteration 811: Loss = 0.4894994307692172\n",
      "Iteration 812: Loss = 0.4893979986902565\n",
      "Iteration 813: Loss = 0.4892970755364763\n",
      "Iteration 814: Loss = 0.48919665875439694\n",
      "Iteration 815: Loss = 0.4890967458033485\n",
      "Iteration 816: Loss = 0.488997334155411\n",
      "Iteration 817: Loss = 0.4888984212953461\n",
      "Iteration 818: Loss = 0.48880000472053675\n",
      "Iteration 819: Loss = 0.4887020819409211\n",
      "Iteration 820: Loss = 0.48860465047893265\n",
      "Iteration 821: Loss = 0.4885077078694341\n",
      "Iteration 822: Loss = 0.488411251659659\n",
      "Iteration 823: Loss = 0.4883152794091446\n",
      "Iteration 824: Loss = 0.48821978868967514\n",
      "Iteration 825: Loss = 0.488124777085217\n",
      "Iteration 826: Loss = 0.488030242191859\n",
      "Iteration 827: Loss = 0.4879361816177526\n",
      "Iteration 828: Loss = 0.48784259298304733\n",
      "Iteration 829: Loss = 0.4877494739198359\n",
      "Iteration 830: Loss = 0.4876568220720917\n",
      "Iteration 831: Loss = 0.48756463509560666\n",
      "Iteration 832: Loss = 0.4874729106579385\n",
      "Iteration 833: Loss = 0.4873816464383438\n",
      "Iteration 834: Loss = 0.4872908401277252\n",
      "Iteration 835: Loss = 0.4872004894285715\n",
      "Iteration 836: Loss = 0.48711059205489776\n",
      "Iteration 837: Loss = 0.4870211457321902\n",
      "Iteration 838: Loss = 0.48693214819734526\n",
      "Iteration 839: Loss = 0.48684359719861536\n",
      "Iteration 840: Loss = 0.4867554904955517\n",
      "Iteration 841: Loss = 0.4866678258589452\n",
      "Iteration 842: Loss = 0.4865806010707718\n",
      "Iteration 843: Loss = 0.48649381392413726\n",
      "Iteration 844: Loss = 0.48640746222321873\n",
      "Iteration 845: Loss = 0.4863215437832123\n",
      "Iteration 846: Loss = 0.4862360564302748\n",
      "Iteration 847: Loss = 0.4861509980014706\n",
      "Iteration 848: Loss = 0.4860663663447161\n",
      "Iteration 849: Loss = 0.4859821593187257\n",
      "Iteration 850: Loss = 0.48589837479295817\n",
      "Iteration 851: Loss = 0.4858150106475605\n",
      "Iteration 852: Loss = 0.48573206477331854\n",
      "Iteration 853: Loss = 0.4856495350715968\n",
      "Iteration 854: Loss = 0.48556741945429355\n",
      "Iteration 855: Loss = 0.4854857158437808\n",
      "Iteration 856: Loss = 0.4854044221728579\n",
      "Iteration 857: Loss = 0.48532353638469294\n",
      "Iteration 858: Loss = 0.4852430564327767\n",
      "Iteration 859: Loss = 0.4851629802808657\n",
      "Iteration 860: Loss = 0.48508330590293575\n",
      "Iteration 861: Loss = 0.48500403128312497\n",
      "Iteration 862: Loss = 0.48492515441568884\n",
      "Iteration 863: Loss = 0.48484667330494474\n",
      "Iteration 864: Loss = 0.48476858596522315\n",
      "Iteration 865: Loss = 0.48469089042081803\n",
      "Iteration 866: Loss = 0.4846135847059368\n",
      "Iteration 867: Loss = 0.48453666686464825\n",
      "Iteration 868: Loss = 0.4844601349508361\n",
      "Iteration 869: Loss = 0.4843839870281489\n",
      "Iteration 870: Loss = 0.4843082211699498\n",
      "Iteration 871: Loss = 0.4842328354592691\n",
      "Iteration 872: Loss = 0.48415782798875473\n",
      "Iteration 873: Loss = 0.48408319686062545\n",
      "Iteration 874: Loss = 0.48400894018662055\n",
      "Iteration 875: Loss = 0.4839350560879556\n",
      "Iteration 876: Loss = 0.4838615426952708\n",
      "Iteration 877: Loss = 0.4837883981485869\n",
      "Iteration 878: Loss = 0.4837156205972562\n",
      "Iteration 879: Loss = 0.48364320819991663\n",
      "Iteration 880: Loss = 0.48357115912444554\n",
      "Iteration 881: Loss = 0.4834994715479112\n",
      "Iteration 882: Loss = 0.48342814365653003\n",
      "Iteration 883: Loss = 0.48335717364561853\n",
      "Iteration 884: Loss = 0.4832865597195472\n",
      "Iteration 885: Loss = 0.483216300091696\n",
      "Iteration 886: Loss = 0.48314639298441087\n",
      "Iteration 887: Loss = 0.4830768366289543\n",
      "Iteration 888: Loss = 0.4830076292654651\n",
      "Iteration 889: Loss = 0.4829387691429111\n",
      "Iteration 890: Loss = 0.48287025451904636\n",
      "Iteration 891: Loss = 0.48280208366036625\n",
      "Iteration 892: Loss = 0.48273425484206367\n",
      "Iteration 893: Loss = 0.4826667663479852\n",
      "Iteration 894: Loss = 0.4825996164705899\n",
      "Iteration 895: Loss = 0.48253280351090155\n",
      "Iteration 896: Loss = 0.4824663257784696\n",
      "Iteration 897: Loss = 0.48240018159132614\n",
      "Iteration 898: Loss = 0.4823343692759416\n",
      "Iteration 899: Loss = 0.4822688871671817\n",
      "Iteration 900: Loss = 0.48220373360826924\n",
      "Iteration 901: Loss = 0.48213890695073847\n",
      "Iteration 902: Loss = 0.4820744055543946\n",
      "Iteration 903: Loss = 0.482010227787273\n",
      "Iteration 904: Loss = 0.4819463720255964\n",
      "Iteration 905: Loss = 0.4818828366537348\n",
      "Iteration 906: Loss = 0.48181962006416595\n",
      "Iteration 907: Loss = 0.4817567206574309\n",
      "Iteration 908: Loss = 0.4816941368420972\n",
      "Iteration 909: Loss = 0.48163186703471705\n",
      "Iteration 910: Loss = 0.4815699096597871\n",
      "Iteration 911: Loss = 0.4815082631497097\n",
      "Iteration 912: Loss = 0.4814469259447514\n",
      "Iteration 913: Loss = 0.48138589649300595\n",
      "Iteration 914: Loss = 0.4813251732503515\n",
      "Iteration 915: Loss = 0.48126475468041596\n",
      "Iteration 916: Loss = 0.48120463925453394\n",
      "Iteration 917: Loss = 0.4811448254517123\n",
      "Iteration 918: Loss = 0.4810853117585865\n",
      "Iteration 919: Loss = 0.4810260966693869\n",
      "Iteration 920: Loss = 0.4809671786858994\n",
      "Iteration 921: Loss = 0.48090855631742446\n",
      "Iteration 922: Loss = 0.4808502280807456\n",
      "Iteration 923: Loss = 0.48079219250008487\n",
      "Iteration 924: Loss = 0.4807344481070714\n",
      "Iteration 925: Loss = 0.48067699344070025\n",
      "Iteration 926: Loss = 0.4806198270472969\n",
      "Iteration 927: Loss = 0.4805629474804815\n",
      "Iteration 928: Loss = 0.4805063533011305\n",
      "Iteration 929: Loss = 0.48045004307734135\n",
      "Iteration 930: Loss = 0.48039401538439563\n",
      "Iteration 931: Loss = 0.4803382688047238\n",
      "Iteration 932: Loss = 0.48028280192786776\n",
      "Iteration 933: Loss = 0.4802276133504473\n",
      "Iteration 934: Loss = 0.4801727016761234\n",
      "Iteration 935: Loss = 0.48011806551556274\n",
      "Iteration 936: Loss = 0.4800637034864029\n",
      "Iteration 937: Loss = 0.48000961421321736\n",
      "Iteration 938: Loss = 0.4799557963274801\n",
      "Iteration 939: Loss = 0.4799022484675337\n",
      "Iteration 940: Loss = 0.47984896927854925\n",
      "Iteration 941: Loss = 0.4797959574124988\n",
      "Iteration 942: Loss = 0.4797432115281153\n",
      "Iteration 943: Loss = 0.4796907302908636\n",
      "Iteration 944: Loss = 0.4796385123729039\n",
      "Iteration 945: Loss = 0.47958655645305703\n",
      "Iteration 946: Loss = 0.479534861216775\n",
      "Iteration 947: Loss = 0.47948342535610405\n",
      "Iteration 948: Loss = 0.4794322475696534\n",
      "Iteration 949: Loss = 0.4793813265625617\n",
      "Iteration 950: Loss = 0.4793306610464654\n",
      "Iteration 951: Loss = 0.47928024973946254\n",
      "Iteration 952: Loss = 0.479230091366086\n",
      "Iteration 953: Loss = 0.47918018465726714\n",
      "Iteration 954: Loss = 0.4791305283503034\n",
      "Iteration 955: Loss = 0.47908112118883034\n",
      "Iteration 956: Loss = 0.4790319619227845\n",
      "Iteration 957: Loss = 0.47898304930837565\n",
      "Iteration 958: Loss = 0.47893438210805545\n",
      "Iteration 959: Loss = 0.478885959090482\n",
      "Iteration 960: Loss = 0.47883777903049424\n",
      "Iteration 961: Loss = 0.4787898407090769\n",
      "Iteration 962: Loss = 0.47874214291333045\n",
      "Iteration 963: Loss = 0.478694684436442\n",
      "Iteration 964: Loss = 0.4786474640776538\n",
      "Iteration 965: Loss = 0.47860048064223304\n",
      "Iteration 966: Loss = 0.47855373294143944\n",
      "Iteration 967: Loss = 0.47850721979249916\n",
      "Iteration 968: Loss = 0.4784609400185713\n",
      "Iteration 969: Loss = 0.47841489244872104\n",
      "Iteration 970: Loss = 0.478369075917888\n",
      "Iteration 971: Loss = 0.47832348926685664\n",
      "Iteration 972: Loss = 0.4782781313422291\n",
      "Iteration 973: Loss = 0.4782330009963934\n",
      "Iteration 974: Loss = 0.47818809708749604\n",
      "Iteration 975: Loss = 0.47814341847941266\n",
      "Iteration 976: Loss = 0.4780989640417193\n",
      "Iteration 977: Loss = 0.47805473264966375\n",
      "Iteration 978: Loss = 0.47801072318413707\n",
      "Iteration 979: Loss = 0.4779669345316453\n",
      "Iteration 980: Loss = 0.47792336558428156\n",
      "Iteration 981: Loss = 0.4778800152396975\n",
      "Iteration 982: Loss = 0.4778368824010759\n",
      "Iteration 983: Loss = 0.4777939659771027\n",
      "Iteration 984: Loss = 0.4777512648819391\n",
      "Iteration 985: Loss = 0.47770877803519485\n",
      "Iteration 986: Loss = 0.47766650436189995\n",
      "Iteration 987: Loss = 0.4776244427924783\n",
      "Iteration 988: Loss = 0.4775825922627206\n",
      "Iteration 989: Loss = 0.4775409517137563\n",
      "Iteration 990: Loss = 0.47749952009202795\n",
      "Iteration 991: Loss = 0.4774582963492644\n",
      "Iteration 992: Loss = 0.47741727944245416\n",
      "Iteration 993: Loss = 0.4773764683338191\n",
      "Iteration 994: Loss = 0.4773358619907866\n",
      "Iteration 995: Loss = 0.47729545938596724\n",
      "Iteration 996: Loss = 0.4772552594971251\n",
      "Iteration 997: Loss = 0.477215261307152\n",
      "Iteration 998: Loss = 0.4771754638040453\n",
      "Iteration 999: Loss = 0.4771358659808793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[3.12718898]]), array([3.37489423]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regression(w, b, x, y, learning_rate=0.01, iter=1000):\n",
    "    for i in range(iter):\n",
    "        y_hat = fits(w, b, x)\n",
    "        dw, db = turunan(y_hat, y, x)\n",
    "        w, b = update_parameter(w, b, dw, db, learning_rate)\n",
    "        loss = loss_function(y_hat, y)\n",
    "        print(f\"Iteration {i}: Loss = {loss}\")\n",
    "    return w, b\n",
    "\n",
    "regression(w, b, x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
